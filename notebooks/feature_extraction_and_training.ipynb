{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1edb31e-09b8-4c69-8a6f-9fa0de2c01ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "Running script...\n"
     ]
    }
   ],
   "source": [
    "# VGGish depends on the following Python packages:\n",
    "# Numpy\n",
    "# Resampy\n",
    "# Tensorflow\n",
    "# Tf_slim\n",
    "# Six\n",
    "# Soundfile\n",
    "#\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "sys.path.append('/Users/pierrekolingba-froidevaux/Desktop/Deep_Learning/dl4ad-group1/models/research/audioset/vggish')\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import resampy  # pylint: disable=import-error\n",
    "import tensorflow.compat.v1 as tf\n",
    "import vggish_input\n",
    "import vggish_params\n",
    "import vggish_postprocess\n",
    "import vggish_slim\n",
    "import os\n",
    "import librosa\n",
    "import glob\n",
    "import tf_slim as slim\n",
    "import vggish_params as params\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "def main(test_mode=False):\n",
    "    print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "    global audio_files\n",
    "    audio_folder_path = '../data/raw_audio_pcm_f32le_16kHz_denoised/'\n",
    "    if test_mode:\n",
    "        audio_files = [\n",
    "            '../data/raw_audio_pcm_f32le_16kHz_denoised/2_99.wav',\n",
    "            '../data/raw_audio_pcm_f32le_16kHz_denoised/1_60.wav',\n",
    "            '../data/raw_audio_pcm_f32le_16kHz_denoised/2_92.wav',\n",
    "            '../data/raw_audio_pcm_f32le_16kHz_denoised/2_97.wav',\n",
    "            '../data/raw_audio_pcm_f32le_16kHz_denoised/1_1185.wav',\n",
    "            '../data/raw_audio_pcm_f32le_16kHz_denoised/2_96.wav',\n",
    "            '../data/raw_audio_pcm_f32le_16kHz_denoised/2_94.wav',\n",
    "            '../data/raw_audio_pcm_f32le_16kHz_denoised/2_91.wav'\n",
    "        ]\n",
    "        print(\"Running script in test mode...\")\n",
    "    else:\n",
    "        audio_files = glob.glob(os.path.join(audio_folder_path, '*.wav'))\n",
    "        print(\"Running script...\")\n",
    "\n",
    "def usage():\n",
    "    \"\"\"\n",
    "    Print usage instructions for the script.\n",
    "    \"\"\"\n",
    "    print(\"Usage:\")\n",
    "    print(\"  python3 feature_extraction.py [test]\")\n",
    "    print()\n",
    "    print(\"Description:\")\n",
    "    print(\"  This script extracts the features from our denoised data.\")\n",
    "    print()\n",
    "    print(\"Optional argument:\")\n",
    "    print(\"  test: Run the script in test mode.\")\n",
    "    print(\"        In test mode, dataset size = 5.\")\n",
    "    print()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Check if the script was executed with the \"--help\" argument\n",
    "    if len(sys.argv) > 1 and sys.argv[1] == \"--help\":\n",
    "        # Display usage instructions and exit\n",
    "        usage()\n",
    "        sys.exit(0)\n",
    "    # Check if the script was executed with the \"test\" argument\n",
    "    elif len(sys.argv) > 1 and sys.argv[1] == \"test\":\n",
    "        # Execute script in test mode\n",
    "        main(test_mode=True)\n",
    "    else:\n",
    "        # Execute script normally\n",
    "        main()\n",
    "\n",
    "\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "if not hasattr(tf.flags, 'DEFINE_string'):\n",
    "    flags = tf.app.flags\n",
    "else:\n",
    "    flags = tf.flags\n",
    "\n",
    "if 'num_batches' not in flags.FLAGS:\n",
    "    flags.DEFINE_integer('num_batches', 30, 'Number of batches of examples to feed into the model.')\n",
    "\n",
    "if 'train_vggish' not in flags.FLAGS:\n",
    "    flags.DEFINE_boolean('train_vggish', True, 'If True, allow VGGish parameters to change during training.')\n",
    "\n",
    "if 'checkpoint' not in flags.FLAGS:\n",
    "    flags.DEFINE_string('checkpoint', 'vggish_model.ckpt', 'Path to the VGGish checkpoint file.')\n",
    "\n",
    "if 'pca_params' not in flags.FLAGS:\n",
    "    flags.DEFINE_string('pca_params', 'vggish_pca_params.npz', 'Path to the VGGish pca params file.')\n",
    "\n",
    "\n",
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babab17a-9015-45ab-bdc1-e38bc596d252",
   "metadata": {},
   "source": [
    "## Mit Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e34dbe0-6374-4a88-961e-a98b64768976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 6406, 1: 8327}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import vggish_input\n",
    "\n",
    "# Chemins vers les fichiers et dossiers nécessaires\n",
    "json_path = '../data/sarcasm_data.json'\n",
    "audio_folder_path = '../data/raw_audio_pcm_f32le_16kHz_denoised/'\n",
    "augmented_folder_path = '../Augmentation/aug_audio_denoised/'  # Chemin vers le dossier contenant les audios augmentés\n",
    "\n",
    "# Chargement des données JSON\n",
    "with open(json_path, 'r') as f:\n",
    "    labels_data = json.load(f)\n",
    "\n",
    "def get_all_examples(audio_folder_path, augmented_folder_path, labels_data):\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Traitement des fichiers audio originaux\n",
    "    for file_name in os.listdir(audio_folder_path):\n",
    "        if file_name.endswith('.wav'):\n",
    "            # Charger l'audio original\n",
    "            audio_path = os.path.join(audio_folder_path, file_name)\n",
    "            audio, sr = librosa.load(audio_path, sr=None)  # Utiliser sr=None pour conserver le taux d'échantillonnage original\n",
    "            examples = vggish_input.waveform_to_examples(audio, sr)\n",
    "\n",
    "            # Obtenir le label du fichier audio à partir des données JSON\n",
    "            original_name = os.path.splitext(file_name)[0]  # Extraire le nom du fichier d'origine sans extension\n",
    "            if original_name in labels_data:\n",
    "                is_sarcastic = labels_data[original_name][\"sarcasm\"]\n",
    "                label = 1 if is_sarcastic else 0  # 1 pour sarcasme, 0 pour non-sarcasme\n",
    "            else:\n",
    "                print(f\"La clé {original_name} n'existe pas dans le fichier JSON.\")\n",
    "                continue\n",
    "\n",
    "            for example in examples:\n",
    "                all_features.append(example)\n",
    "                all_labels.append(label)\n",
    "\n",
    "    # Traitement des fichiers audio augmentés\n",
    "    for file_name in os.listdir(augmented_folder_path):\n",
    "        if file_name.endswith('.wav'):\n",
    "            # Charger l'audio augmenté\n",
    "            augmented_audio_path = os.path.join(augmented_folder_path, file_name)\n",
    "            audio, sr = librosa.load(augmented_audio_path, sr=None)\n",
    "            examples = vggish_input.waveform_to_examples(audio, sr)\n",
    "\n",
    "            # Obtenir le label du fichier audio à partir des données JSON\n",
    "            original_name = \"_\".join(file_name.split('_')[:-1])  # Extraire le nom du fichier d'origine\n",
    "            if original_name in labels_data:\n",
    "                is_sarcastic = labels_data[original_name][\"sarcasm\"]\n",
    "                label = 1 if is_sarcastic else 0  # 1 pour sarcasme, 0 pour non-sarcasme\n",
    "            else:\n",
    "                print(f\"La clé {original_name} n'existe pas dans le fichier JSON.\")\n",
    "                continue\n",
    "\n",
    "            for example in examples:\n",
    "                all_features.append(example)\n",
    "                all_labels.append(label)\n",
    "\n",
    "    return np.array(all_features), np.array(all_labels)\n",
    "\n",
    "# Exécuter la fonction sur les fichiers originaux et augmentés\n",
    "features, labels = get_all_examples(audio_folder_path, augmented_folder_path, labels_data)\n",
    "# 'labels' est le tableau de vos labels\n",
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a155df8-498d-43e1-b4ff-8d35d86193df",
   "metadata": {},
   "source": [
    "## Ohne Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c706ca35-028a-49b7-861b-46e03ba62cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import vggish_input\n",
    "\n",
    "# Chemins vers les fichiers et dossiers nécessaires\n",
    "json_path = '../data/sarcasm_data.json'\n",
    "audio_folder_path = '../data/raw_audio_pcm_f32le_16kHz_denoised/'\n",
    "\n",
    "# Chargement des données JSON\n",
    "with open(json_path, 'r') as f:\n",
    "    labels_data = json.load(f)\n",
    "    \n",
    "def get_all_examples(audio_folder_path, labels_data, audio_files):\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "\n",
    "    for file_path in audio_files:\n",
    "        file_name = os.path.basename(file_path).replace('.wav', '')\n",
    "\n",
    "        # Charger l'audio et le convertir en exemple\n",
    "        audio, sr = librosa.load(file_path, sr=22050)\n",
    "        examples = vggish_input.waveform_to_examples(audio, sr)\n",
    "\n",
    "        # Obtenir le label du fichier audio depuis les données JSON\n",
    "        is_sarcastic = labels_data[file_name][\"sarcasm\"]\n",
    "        label = 1 if is_sarcastic else 0  # Label binaire: 1 pour sarcastique, 0 pour non sarcastique\n",
    "\n",
    "        for example in examples:\n",
    "            all_features.append(example)\n",
    "            all_labels.append(label)\n",
    "\n",
    "    return np.array(all_features), np.array(all_labels)\n",
    "\n",
    "# Vous devrez fournir la liste des chemins de fichiers audio à la fonction\n",
    "features, labels = get_all_examples(audio_folder_path, labels_data, audio_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09310bbc-c1ab-4b30-af57-776ef0551ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1505, 1: 1960}\n"
     ]
    }
   ],
   "source": [
    "# 'labels' est le tableau de vos labels\n",
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb647086-eba2-42ea-9380-bf97d4381a33",
   "metadata": {},
   "source": [
    "### Dauer der gesamten Aufnahmen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "353e8951-4fb7-4aab-9def-5470529a0c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Durée totale des fichiers audio sarcastiques: 2021.4168125 secondes\n",
      "Durée totale des fichiers audio non sarcastiques: 1580.9109374999978 secondes\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import librosa\n",
    "\n",
    "# Chemins vers les fichiers et dossiers nécessaires\n",
    "json_path = '../data/sarcasm_data.json'\n",
    "audio_folder_path = '../data/raw_audio_pcm_f32le_16kHz_denoised/'\n",
    "\n",
    "# Chargement des données JSON\n",
    "with open(json_path, 'r') as f:\n",
    "    labels_data = json.load(f)\n",
    "\n",
    "def calculate_durations(audio_folder_path, labels_data, audio_files):\n",
    "    total_duration_sarcastic = 0.0  # Durée totale des audios sarcastiques\n",
    "    total_duration_non_sarcastic = 0.0  # Durée totale des audios non sarcastiques\n",
    "\n",
    "    for file_path in audio_files:\n",
    "        file_name = os.path.basename(file_path).replace('.wav', '')\n",
    "\n",
    "        # Charger l'audio pour obtenir sa durée\n",
    "        audio, sr = librosa.load(file_path, sr=None)\n",
    "        duration = len(audio) / sr\n",
    "\n",
    "        # Obtenir le label du fichier audio depuis les données JSON\n",
    "        is_sarcastic = labels_data[file_name][\"sarcasm\"]\n",
    "        if is_sarcastic:\n",
    "            total_duration_sarcastic += duration\n",
    "        else:\n",
    "            total_duration_non_sarcastic += duration\n",
    "\n",
    "    return total_duration_sarcastic, total_duration_non_sarcastic\n",
    "\n",
    "# Génération de la liste des chemins de fichiers audio (exemplaire, à ajuster selon votre situation)\n",
    "audio_files = [os.path.join(audio_folder_path, file) for file in os.listdir(audio_folder_path) if file.endswith('.wav')]\n",
    "\n",
    "total_duration_sarcastic, total_duration_non_sarcastic = calculate_durations(audio_folder_path, labels_data, audio_files)\n",
    "\n",
    "print(f\"Durée totale des fichiers audio sarcastiques: {total_duration_sarcastic} secondes\")\n",
    "print(f\"Durée totale des fichiers audio non sarcastiques: {total_duration_non_sarcastic} secondes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a3b177-b228-4123-8677-c1993204db53",
   "metadata": {},
   "source": [
    "# Feature extraction Embedding with Resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a75e53-d35e-4420-b9ea-de2ed7e08c04",
   "metadata": {},
   "source": [
    "## Balancing of classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7474668-4d27-4aef-95c4-ce1ea5e38d05",
   "metadata": {},
   "source": [
    "### Balancing by increasing sample size of the smaller class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b2f2aa0e-8f4d-4a1f-95c3-d564bde93164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imbalanced-learn\n",
      "  Downloading imbalanced_learn-0.12.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/pierrekolingba-froidevaux/anaconda3/envs/new_env/lib/python3.11/site-packages (from imbalanced-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Users/pierrekolingba-froidevaux/anaconda3/envs/new_env/lib/python3.11/site-packages (from imbalanced-learn) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /Users/pierrekolingba-froidevaux/anaconda3/envs/new_env/lib/python3.11/site-packages (from imbalanced-learn) (1.2.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/pierrekolingba-froidevaux/anaconda3/envs/new_env/lib/python3.11/site-packages (from imbalanced-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/pierrekolingba-froidevaux/anaconda3/envs/new_env/lib/python3.11/site-packages (from imbalanced-learn) (3.2.0)\n",
      "Downloading imbalanced_learn-0.12.0-py3-none-any.whl (257 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m257.7/257.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: imbalanced-learn\n",
      "Successfully installed imbalanced-learn-0.12.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "982220ea-108d-47be-8e9f-d2c34ef72ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nouvelle répartition des classes après suréchantillonnage: {0: 1960, 1: 1960}\n",
      "(array([0, 1]), array([1960, 1960]))\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import numpy as np\n",
    "\n",
    "# Initialisation de RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "\n",
    "# Appliquer le suréchantillonnage sur les labels pour obtenir les indices des données suréchantillonnées\n",
    "# Ici, nous avons besoin d'aplatir les features si elles sont en 3D pour VGGish\n",
    "features_flattened = features.reshape(features.shape[0], -1)\n",
    "features_oversampled, labels_oversampled = ros.fit_resample(features_flattened, labels)\n",
    "\n",
    "# Reformer les features suréchantillonnées pour correspondre au format attendu par VGGish si nécessaire\n",
    "features_oversampled = features_oversampled.reshape(-1, vggish_params.NUM_FRAMES, vggish_params.NUM_BANDS)\n",
    "\n",
    "# Vérifier la nouvelle distribution des classes après suréchantillonnage\n",
    "unique, counts = np.unique(labels_oversampled, return_counts=True)\n",
    "print(\"Nouvelle répartition des classes après suréchantillonnage:\", dict(zip(unique, counts)))\n",
    "# Assurez-vous que les données sous-échantillonnées contiennent bien des instances des deux classes\n",
    "print(np.unique(labels_oversampled, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e85c88f1-85be-4f41-9146-50c576791887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./vggish_model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pierrekolingba-froidevaux/anaconda3/envs/new_env/lib/python3.11/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1697: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n",
      "/Users/pierrekolingba-froidevaux/anaconda3/envs/new_env/lib/python3.11/site-packages/tensorflow/python/keras/legacy_tf_layers/core.py:318: UserWarning: `tf.layers.flatten` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Flatten` instead.\n",
      "  warnings.warn('`tf.layers.flatten` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Répartition des classes dans l'ensemble d'entraînement: {0: 1568, 1: 1568}\n",
      "Répartition des classes dans l'ensemble de test: {0: 392, 1: 392}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import vggish_slim, vggish_params, vggish_postprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def extract_embeddings(features, checkpoint_path, pca_params_path):\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "    pproc = vggish_postprocess.Postprocessor(pca_params_path)\n",
    "    with tf.compat.v1.Graph().as_default(), tf.compat.v1.Session() as sess:\n",
    "        vggish_slim.define_vggish_slim(training=False)\n",
    "        vggish_slim.load_vggish_slim_checkpoint(sess, checkpoint_path)\n",
    "        features_tensor = sess.graph.get_tensor_by_name(vggish_params.INPUT_TENSOR_NAME)\n",
    "        embedding_tensor = sess.graph.get_tensor_by_name(vggish_params.OUTPUT_TENSOR_NAME)\n",
    "        \n",
    "        # Assurez-vous que la taille des features correspond à l'attente du réseau\n",
    "        features_reshaped = features.reshape((-1, vggish_params.NUM_FRAMES, vggish_params.NUM_BANDS))\n",
    "        \n",
    "        [embedding_batch] = sess.run([embedding_tensor], feed_dict={features_tensor: features_reshaped})\n",
    "        postprocessed_batch = pproc.postprocess(embedding_batch)\n",
    "        return postprocessed_batch\n",
    "\n",
    "# Chemins vers les fichiers nécessaires\n",
    "checkpoint_path = './vggish_model.ckpt'\n",
    "pca_params_path = './vggish_pca_params.npz'\n",
    "\n",
    "# Utilisation de `features_oversampled` et `labels_oversampled` préparés précédemment\n",
    "\n",
    "# Extraction des embeddings\n",
    "embedding_batch_oversampled = extract_embeddings(features_oversampled, checkpoint_path, pca_params_path)\n",
    "\n",
    "# Division des données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(embedding_batch_oversampled, labels_oversampled, test_size=0.2, random_state=42, stratify=labels_oversampled)\n",
    "\n",
    "# Affichage de la répartition des classes\n",
    "unique_train, counts_train = np.unique(y_train, return_counts=True)\n",
    "unique_test, counts_test = np.unique(y_test, return_counts=True)\n",
    "print(\"Répartition des classes dans l'ensemble d'entraînement:\", dict(zip(unique_train, counts_train)))\n",
    "print(\"Répartition des classes dans l'ensemble de test:\", dict(zip(unique_test, counts_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965c9ab9-95b8-4706-9335-acbc189a138f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Balancing by decreasing sample size of the bigger class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef0944e7-74c0-4d55-b8d0-011b0951f068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1]), array([1505, 1505]))\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "# Le reshape est utilisé ici pour transformer les labels en une structure acceptable par fit_resample\n",
    "labels_resampled_indices = rus.fit_resample(np.arange(labels.shape[0]).reshape(-1, 1), labels)[0]\n",
    "\n",
    "# Utilisez les indices pour sélectionner les observations correspondantes dans features\n",
    "features_undersampled = features[labels_resampled_indices.flatten()]\n",
    "labels_undersampled = labels[labels_resampled_indices.flatten()]\n",
    "\n",
    "# Assurez-vous que les données sous-échantillonnées contiennent bien des instances des deux classes\n",
    "print(np.unique(labels_undersampled, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0da31acc-d8ef-49ed-bad2-4be44b2d0d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./vggish_model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pierrekolingba-froidevaux/anaconda3/envs/new_env/lib/python3.11/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1697: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n",
      "/Users/pierrekolingba-froidevaux/anaconda3/envs/new_env/lib/python3.11/site-packages/tensorflow/python/keras/legacy_tf_layers/core.py:318: UserWarning: `tf.layers.flatten` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Flatten` instead.\n",
      "  warnings.warn('`tf.layers.flatten` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Répartition des classes dans l'ensemble d'entraînement: {0: 1204, 1: 1204}\n",
      "Répartition des classes dans l'ensemble de test: {0: 301, 1: 301}\n"
     ]
    }
   ],
   "source": [
    "# Script 2: Extraction des Embeddings et Préparation des Données\n",
    "\n",
    "import tensorflow as tf\n",
    "import vggish_slim, vggish_params, vggish_postprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def extract_embeddings(features, checkpoint_path, pca_params_path):\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "    pproc = vggish_postprocess.Postprocessor(pca_params_path)\n",
    "    with tf.compat.v1.Graph().as_default(), tf.compat.v1.Session() as sess:\n",
    "        vggish_slim.define_vggish_slim(training=False)\n",
    "        vggish_slim.load_vggish_slim_checkpoint(sess, checkpoint_path)\n",
    "        features_tensor = sess.graph.get_tensor_by_name(vggish_params.INPUT_TENSOR_NAME)\n",
    "        embedding_tensor = sess.graph.get_tensor_by_name(vggish_params.OUTPUT_TENSOR_NAME)\n",
    "        \n",
    "        # Adapter la taille des features pour correspondre à l'attente du réseau\n",
    "        features_reshaped = features.reshape((-1, vggish_params.NUM_FRAMES, vggish_params.NUM_BANDS))\n",
    "        \n",
    "        [embedding_batch] = sess.run([embedding_tensor], feed_dict={features_tensor: features_reshaped})\n",
    "        postprocessed_batch = pproc.postprocess(embedding_batch)\n",
    "        return postprocessed_batch\n",
    "\n",
    "# Chemins vers les fichiers nécessaires\n",
    "checkpoint_path = './vggish_model.ckpt'\n",
    "pca_params_path = './vggish_pca_params.npz'\n",
    "\n",
    "# Assurez-vous que `features_undersampled` et `labels_undersampled` sont définis\n",
    "# Ces variables devraient déjà être disponibles en mémoire si le script de sous-échantillonnage a été exécuté auparavant dans la même session\n",
    "\n",
    "# Extraction des embeddings\n",
    "embedding_batch = extract_embeddings(features_undersampled, checkpoint_path, pca_params_path)\n",
    "\n",
    "# Division des données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(embedding_batch, labels_undersampled, test_size=0.2, random_state=42, stratify=labels_undersampled)\n",
    "\n",
    "# Affichage de la répartition des classes\n",
    "unique_train, counts_train = np.unique(y_train, return_counts=True)\n",
    "unique_test, counts_test = np.unique(y_test, return_counts=True)\n",
    "print(\"Répartition des classes dans l'ensemble d'entraînement:\", dict(zip(unique_train, counts_train)))\n",
    "print(\"Répartition des classes dans l'ensemble de test:\", dict(zip(unique_test, counts_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba9e956-b594-4260-8030-1a1a5695c2e5",
   "metadata": {},
   "source": [
    "# Feature extraction Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "16eb029a-3125-4758-9270-6bfcf67f6b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./vggish_model.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pierrekolingba-froidevaux/anaconda3/envs/new_env/lib/python3.11/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1697: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n",
      "/Users/pierrekolingba-froidevaux/anaconda3/envs/new_env/lib/python3.11/site-packages/tensorflow/python/keras/legacy_tf_layers/core.py:318: UserWarning: `tf.layers.flatten` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Flatten` instead.\n",
      "  warnings.warn('`tf.layers.flatten` is deprecated and '\n",
      "INFO:tensorflow:Restoring parameters from ./vggish_model.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.8820404  -0.00874145  0.38078845 ... -0.6591561   0.06713051\n",
      "  -0.36917007]\n",
      " [-0.43646303  0.05022906  0.11660872 ... -0.4471433  -0.04523692\n",
      "  -0.30223668]\n",
      " [ 0.29241273 -0.17487231 -0.09087229 ...  0.18623087 -0.04541071\n",
      "  -0.45185494]\n",
      " ...\n",
      " [-0.25952086 -0.20381099  0.17393252 ... -0.06592399 -0.00994544\n",
      "  -0.19809008]\n",
      " [ 0.23226415  0.1085295  -0.03712422 ...  0.23585996  0.05718157\n",
      "  -0.3611402 ]\n",
      " [-0.11748618 -0.10055816  0.27917695 ...  0.11004415  0.00833523\n",
      "  -0.40331632]]\n",
      "[[ 80 101  64 ... 118 255 168]\n",
      " [ 93  78 132 ... 229 255 207]\n",
      " [ 37  88 153 ... 246 218   0]\n",
      " ...\n",
      " [121  57 146 ...  97 159 203]\n",
      " [ 53  73 189 ...  47   4  86]\n",
      " [ 87  44 169 ...  44 214 255]]\n",
      "Répartition des classes dans l'ensemble d'entraînement: {0: 1204, 1: 1565}\n",
      "Répartition des classes dans l'ensemble de test: {0: 301, 1: 392}\n"
     ]
    }
   ],
   "source": [
    "def extract_embeddings(features, checkpoint_path, pca_params_path):\n",
    "    import tensorflow as tf\n",
    "    import vggish_slim, vggish_params, vggish_postprocess\n",
    "\n",
    "    # Use TensorFlow 1.x compatibility mode\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "    \n",
    "    # Initialize the post-processor\n",
    "    pproc = vggish_postprocess.Postprocessor(pca_params_path)\n",
    "\n",
    "    with tf.compat.v1.Graph().as_default(), tf.compat.v1.Session() as sess:\n",
    "        # Initialize VGGish and load the checkpoint\n",
    "        vggish_slim.define_vggish_slim(training=False)\n",
    "        vggish_slim.load_vggish_slim_checkpoint(sess, checkpoint_path)\n",
    "\n",
    "        # Locate input and output tensors\n",
    "        features_tensor = sess.graph.get_tensor_by_name(vggish_params.INPUT_TENSOR_NAME)\n",
    "        embedding_tensor = sess.graph.get_tensor_by_name(vggish_params.OUTPUT_TENSOR_NAME)\n",
    "\n",
    "        # Run the model to obtain embeddings\n",
    "        [embedding_batch] = sess.run([embedding_tensor], feed_dict={features_tensor: features})\n",
    "        print(embedding_batch)\n",
    "\n",
    "        # Apply post-processing - PCA (whitens the data)\n",
    "        postprocessed_batch = pproc.postprocess(embedding_batch)\n",
    "        print(postprocessed_batch)\n",
    "        \n",
    "        return postprocessed_batch\n",
    "\n",
    "# Example usage (make sure to replace 'checkpoint_path' and 'pca_params_path' with the actual paths to your files)\n",
    "checkpoint_path = './vggish_model.ckpt'\n",
    "pca_params_path = './vggish_pca_params.npz'\n",
    "embedding_batch = extract_embeddings(features, checkpoint_path, pca_params_path)\n",
    "\n",
    "labels = np.array(labels)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Diviser les données tout en maintenant la répartition des classes\n",
    "X_train, X_test, y_train, y_test = train_test_split(embedding_batch, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "\n",
    "# Vérifier la répartition des classes dans l'ensemble d'entraînement et de test\n",
    "unique_train, counts_train = np.unique(y_train, return_counts=True)\n",
    "unique_test, counts_test = np.unique(y_test, return_counts=True)\n",
    "\n",
    "print(\"Répartition des classes dans l'ensemble d'entraînement:\", dict(zip(unique_train, counts_train)))\n",
    "print(\"Répartition des classes dans l'ensemble de test:\", dict(zip(unique_test, counts_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "47e830af-5b6d-491e-aa58-013f03447d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1505, 1: 1957}\n"
     ]
    }
   ],
   "source": [
    "# 'labels' est le tableau de vos labels\n",
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc08396e-071e-4e36-aeb9-5d22cd84eb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille de l'ensemble d'entraînement: 2769\n",
      "Taille de l'ensemble de test: 693\n"
     ]
    }
   ],
   "source": [
    "print(\"Taille de l'ensemble d'entraînement:\", len(X_train))\n",
    "print(\"Taille de l'ensemble de test:\", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495021e4-96fd-4cbf-b12c-6b73a6069c73",
   "metadata": {},
   "source": [
    "# Training with Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5b1fac-aaed-42d1-bb61-0f7e1309d5cb",
   "metadata": {},
   "source": [
    "## Mit Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ee693e-1327-4daf-8c18-952c37fae31f",
   "metadata": {},
   "source": [
    "### training with oversampled dataset from smaller class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4299f334-5f02-43a1-a511-fd7dbf492e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2508 samples, validate on 628 samples\n",
      "Epoch 1/50\n",
      "  32/2508 [..............................] - ETA: 3s - loss: 7.9668 - accuracy: 0.3438"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-11 17:28:38.283330: W tensorflow/c/c_api.cc:305] Operation '{name:'training_52/Adam/dense_52/kernel/v/Assign' id:9626 op device:{requested: '', assigned: ''} def:{{{node training_52/Adam/dense_52/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_52/Adam/dense_52/kernel/v, training_52/Adam/dense_52/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2144/2508 [========================>.....] - ETA: 0s - loss: 115.0883 - accuracy: 0.5014\n",
      "Epoch 1: val_loss improved from inf to 2.34075, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pierrekolingba-froidevaux/anaconda3/envs/new_env/lib/python3.11/site-packages/keras/src/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n",
      "2024-03-11 17:28:38.572864: W tensorflow/c/c_api.cc:305] Operation '{name:'loss_26/mul' id:9471 op device:{requested: '', assigned: ''} def:{{{node loss_26/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_26/mul/x, loss_26/dense_53_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "/Users/pierrekolingba-froidevaux/anaconda3/envs/new_env/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2508/2508 [==============================] - 0s 162us/sample - loss: 98.8096 - accuracy: 0.5120 - val_loss: 2.3408 - val_accuracy: 0.4857\n",
      "Epoch 2/50\n",
      "2016/2508 [=======================>......] - ETA: 0s - loss: 1.0982 - accuracy: 0.5169\n",
      "Epoch 2: val_loss improved from 2.34075 to 0.75070, saving model to best_model.h5\n",
      "2508/2508 [==============================] - 0s 64us/sample - loss: 1.0945 - accuracy: 0.5163 - val_loss: 0.7507 - val_accuracy: 0.4809\n",
      "Epoch 3/50\n",
      "2432/2508 [============================>.] - ETA: 0s - loss: 0.6749 - accuracy: 0.5855\n",
      "Epoch 3: val_loss improved from 0.75070 to 0.68193, saving model to best_model.h5\n",
      "2508/2508 [==============================] - 0s 72us/sample - loss: 0.6758 - accuracy: 0.5829 - val_loss: 0.6819 - val_accuracy: 0.5796\n",
      "Epoch 4/50\n",
      "2016/2508 [=======================>......] - ETA: 0s - loss: 0.6672 - accuracy: 0.5967\n",
      "Epoch 4: val_loss did not improve from 0.68193\n",
      "2508/2508 [==============================] - 0s 57us/sample - loss: 0.6699 - accuracy: 0.5913 - val_loss: 0.6829 - val_accuracy: 0.5717\n",
      "Epoch 5/50\n",
      "1888/2508 [=====================>........] - ETA: 0s - loss: 0.6629 - accuracy: 0.5906\n",
      "Epoch 5: val_loss did not improve from 0.68193\n",
      "2508/2508 [==============================] - 0s 61us/sample - loss: 0.6664 - accuracy: 0.5877 - val_loss: 0.6829 - val_accuracy: 0.5605\n",
      "Epoch 6/50\n",
      "1824/2508 [====================>.........] - ETA: 0s - loss: 0.6586 - accuracy: 0.5894\n",
      "Epoch 6: val_loss did not improve from 0.68193\n",
      "2508/2508 [==============================] - 0s 60us/sample - loss: 0.6627 - accuracy: 0.5801 - val_loss: 0.6942 - val_accuracy: 0.5111\n",
      "Epoch 7/50\n",
      "2016/2508 [=======================>......] - ETA: 0s - loss: 0.6684 - accuracy: 0.5838\n",
      "Epoch 7: val_loss did not improve from 0.68193\n",
      "2508/2508 [==============================] - 0s 59us/sample - loss: 0.6662 - accuracy: 0.5901 - val_loss: 0.6965 - val_accuracy: 0.5557\n",
      "Epoch 8/50\n",
      "1824/2508 [====================>.........] - ETA: 0s - loss: 0.6661 - accuracy: 0.5784\n",
      "Epoch 8: val_loss did not improve from 0.68193\n",
      "2508/2508 [==============================] - 0s 62us/sample - loss: 0.6664 - accuracy: 0.5801 - val_loss: 0.6886 - val_accuracy: 0.5430\n",
      "Epoch 9/50\n",
      "1920/2508 [=====================>........] - ETA: 0s - loss: 0.6724 - accuracy: 0.5802\n",
      "Epoch 9: val_loss did not improve from 0.68193\n",
      "2508/2508 [==============================] - 0s 59us/sample - loss: 0.6700 - accuracy: 0.5873 - val_loss: 0.6894 - val_accuracy: 0.5876\n",
      "Epoch 10/50\n",
      "1824/2508 [====================>.........] - ETA: 0s - loss: 0.6667 - accuracy: 0.5894\n",
      "Epoch 10: val_loss did not improve from 0.68193\n",
      "2508/2508 [==============================] - 0s 60us/sample - loss: 0.6683 - accuracy: 0.5865 - val_loss: 0.7226 - val_accuracy: 0.4984\n",
      "Epoch 11/50\n",
      "1824/2508 [====================>.........] - ETA: 0s - loss: 0.6625 - accuracy: 0.5861\n",
      "Epoch 11: val_loss did not improve from 0.68193\n",
      "2508/2508 [==============================] - 0s 62us/sample - loss: 0.6614 - accuracy: 0.5949 - val_loss: 0.6979 - val_accuracy: 0.5287\n",
      "Epoch 12/50\n",
      "1824/2508 [====================>.........] - ETA: 0s - loss: 0.6726 - accuracy: 0.5713\n",
      "Epoch 12: val_loss did not improve from 0.68193\n",
      "2508/2508 [==============================] - 0s 61us/sample - loss: 0.6706 - accuracy: 0.5789 - val_loss: 0.6851 - val_accuracy: 0.5796\n",
      "Epoch 13/50\n",
      "1888/2508 [=====================>........] - ETA: 0s - loss: 0.6646 - accuracy: 0.6001\n",
      "Epoch 13: val_loss did not improve from 0.68193\n",
      "2508/2508 [==============================] - 0s 61us/sample - loss: 0.6649 - accuracy: 0.6041 - val_loss: 0.6948 - val_accuracy: 0.5510\n",
      "Epoch 13: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.54      0.56       392\n",
      "           1       0.57      0.62      0.60       392\n",
      "\n",
      "    accuracy                           0.58       784\n",
      "   macro avg       0.58      0.58      0.58       784\n",
      "weighted avg       0.58      0.58      0.58       784\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pierrekolingba-froidevaux/anaconda3/envs/new_env/lib/python3.11/site-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "2024-03-11 17:28:40.770080: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_53/Sigmoid' id:9414 op device:{requested: '', assigned: ''} def:{{{node dense_53/Sigmoid}} = Sigmoid[T=DT_FLOAT, _has_manual_control_dependencies=true](dense_53/BiasAdd)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "_NUM_CLASSES = 1  # Pour une classification binaire, utilisez 1 unité de sortie\n",
    "\n",
    "def train_and_evaluate_model(X_train, y_train, X_test, y_test, epochs, batch_size):\n",
    "    # Création du modèle\n",
    "    model = Sequential([\n",
    "        Dense(4096, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        Dense(_NUM_CLASSES, activation='sigmoid')  # Utilisez 'sigmoid' pour la classification binaire\n",
    "    ])\n",
    "\n",
    "    # Compilation du modèle\n",
    "    optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.01)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='binary_crossentropy',  # 'binary_crossentropy' pour la classification binaire\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Configuration des callbacks\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min')\n",
    "    model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, mode='min', verbose=1)\n",
    "\n",
    "    # Entraînement du modèle\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "    # Chargement du meilleur modèle sauvegardé\n",
    "    model.load_weights('best_model.h5')\n",
    "\n",
    "    # Évaluation du modèle\n",
    "    predictions = model.predict(X_test)\n",
    "    predictions = np.round(predictions)\n",
    "    \n",
    "    print(classification_report(y_test, predictions))\n",
    "\n",
    "# Exemple d'utilisation\n",
    "# Assurez-vous que 'embedding_batch_oversampled' et 'labels_oversampled' ont été correctement préparés auparavant\n",
    "X_train, X_test, y_train, y_test = train_test_split(embedding_batch_oversampled, labels_oversampled, test_size=0.2, stratify=labels_oversampled) #, random_state=42,\n",
    "\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "\n",
    "train_and_evaluate_model(X_train, y_train, X_test, y_test, epochs, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b332836a-ff29-4ccd-9e6d-4f87b6b47cfd",
   "metadata": {},
   "source": [
    "### undersampling from the bigger class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c910087-1d6c-47fa-9ff8-887e5855e9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1926 samples, validate on 482 samples\n",
      "Epoch 1/10\n",
      "1312/1926 [===================>..........] - ETA: 0s - loss: 47.4878 - accuracy: 0.5023"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-11 16:28:01.784636: W tensorflow/c/c_api.cc:305] Operation '{name:'training_20/Adam/beta_2/Assign' id:3823 op device:{requested: '', assigned: ''} def:{{{node training_20/Adam/beta_2/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_20/Adam/beta_2, training_20/Adam/beta_2/Initializer/initial_value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "/Users/pierrekolingba-froidevaux/anaconda3/envs/new_env/lib/python3.11/site-packages/keras/src/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n",
      "2024-03-11 16:28:01.950028: W tensorflow/c/c_api.cc:305] Operation '{name:'loss_10/mul' id:3711 op device:{requested: '', assigned: ''} def:{{{node loss_10/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_10/mul/x, loss_10/dense_21_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 4.79783, saving model to best_model.h5\n",
      "1926/1926 [==============================] - 0s 114us/sample - loss: 33.9400 - accuracy: 0.5088 - val_loss: 4.7978 - val_accuracy: 0.4896\n",
      "Epoch 2/10\n",
      "1088/1926 [===============>..............] - ETA: 0s - loss: 3.8138 - accuracy: 0.5211\n",
      "Epoch 2: val_loss did not improve from 4.79783\n",
      "1926/1926 [==============================] - 0s 51us/sample - loss: 3.2830 - accuracy: 0.5389 - val_loss: 5.5430 - val_accuracy: 0.5456\n",
      "Epoch 3/10\n",
      "  32/1926 [..............................] - ETA: 0s - loss: 7.2752 - accuracy: 0.3125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pierrekolingba-froidevaux/anaconda3/envs/new_env/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1120/1926 [================>.............] - ETA: 0s - loss: 2.8479 - accuracy: 0.5536\n",
      "Epoch 3: val_loss improved from 4.79783 to 3.63407, saving model to best_model.h5\n",
      "1926/1926 [==============================] - 0s 54us/sample - loss: 2.7647 - accuracy: 0.5395 - val_loss: 3.6341 - val_accuracy: 0.4627\n",
      "Epoch 4/10\n",
      "1152/1926 [================>.............] - ETA: 0s - loss: 1.4722 - accuracy: 0.5964\n",
      "Epoch 4: val_loss improved from 3.63407 to 1.86599, saving model to best_model.h5\n",
      "1926/1926 [==============================] - 0s 52us/sample - loss: 1.8573 - accuracy: 0.5815 - val_loss: 1.8660 - val_accuracy: 0.5290\n",
      "Epoch 5/10\n",
      " 832/1926 [===========>..................] - ETA: 0s - loss: 1.9517 - accuracy: 0.5745\n",
      "Epoch 5: val_loss did not improve from 1.86599\n",
      "1926/1926 [==============================] - 0s 59us/sample - loss: 1.7660 - accuracy: 0.5805 - val_loss: 2.8146 - val_accuracy: 0.4959\n",
      "Epoch 6/10\n",
      "1152/1926 [================>.............] - ETA: 0s - loss: 1.7670 - accuracy: 0.6033\n",
      "Epoch 6: val_loss improved from 1.86599 to 1.69780, saving model to best_model.h5\n",
      "1926/1926 [==============================] - 0s 54us/sample - loss: 1.8675 - accuracy: 0.5763 - val_loss: 1.6978 - val_accuracy: 0.5290\n",
      "Epoch 7/10\n",
      "1120/1926 [================>.............] - ETA: 0s - loss: 1.3561 - accuracy: 0.6179\n",
      "Epoch 7: val_loss improved from 1.69780 to 1.55547, saving model to best_model.h5\n",
      "1926/1926 [==============================] - 0s 57us/sample - loss: 1.2040 - accuracy: 0.6189 - val_loss: 1.5555 - val_accuracy: 0.5021\n",
      "Epoch 8/10\n",
      "1088/1926 [===============>..............] - ETA: 0s - loss: 0.8172 - accuracy: 0.6636\n",
      "Epoch 8: val_loss improved from 1.55547 to 1.44272, saving model to best_model.h5\n",
      "1926/1926 [==============================] - 0s 56us/sample - loss: 1.0309 - accuracy: 0.6443 - val_loss: 1.4427 - val_accuracy: 0.5560\n",
      "Epoch 9/10\n",
      "1088/1926 [===============>..............] - ETA: 0s - loss: 1.3441 - accuracy: 0.6241\n",
      "Epoch 9: val_loss improved from 1.44272 to 1.13118, saving model to best_model.h5\n",
      "1926/1926 [==============================] - 0s 56us/sample - loss: 1.3045 - accuracy: 0.6168 - val_loss: 1.1312 - val_accuracy: 0.5892\n",
      "Epoch 10/10\n",
      "1088/1926 [===============>..............] - ETA: 0s - loss: 0.7806 - accuracy: 0.6645\n",
      "Epoch 10: val_loss did not improve from 1.13118\n",
      "1926/1926 [==============================] - 0s 53us/sample - loss: 0.7912 - accuracy: 0.6630 - val_loss: 1.4183 - val_accuracy: 0.4917\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.33      0.44       301\n",
      "           1       0.56      0.85      0.67       301\n",
      "\n",
      "    accuracy                           0.59       602\n",
      "   macro avg       0.62      0.59      0.56       602\n",
      "weighted avg       0.62      0.59      0.56       602\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pierrekolingba-froidevaux/anaconda3/envs/new_env/lib/python3.11/site-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "2024-03-11 16:28:03.060505: W tensorflow/c/c_api.cc:305] Operation '{name:'dense_21/Sigmoid' id:3654 op device:{requested: '', assigned: ''} def:{{{node dense_21/Sigmoid}} = Sigmoid[T=DT_FLOAT, _has_manual_control_dependencies=true](dense_21/BiasAdd)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "_NUM_CLASSES = 1  # Pour une classification binaire, utilisez 1 unité de sortie\n",
    "\n",
    "def train_and_evaluate_model(X_train, y_train, X_test, y_test, epochs, batch_size):\n",
    "    # Création du modèle\n",
    "    model = Sequential([\n",
    "        Dense(4096, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        Dense(_NUM_CLASSES, activation='sigmoid')  # Utilisez 'sigmoid' pour la classification binaire\n",
    "    ])\n",
    "\n",
    "    # Compilation du modèle\n",
    "    optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='binary_crossentropy',  # 'binary_crossentropy' pour la classification binaire\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Configuration des callbacks\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min')\n",
    "    model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, mode='min', verbose=1)\n",
    "\n",
    "    # Entraînement du modèle\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "    # Chargement du meilleur modèle sauvegardé\n",
    "    model.load_weights('best_model.h5')\n",
    "\n",
    "    # Évaluation du modèle\n",
    "    predictions = model.predict(X_test)\n",
    "    predictions = np.round(predictions)\n",
    "    \n",
    "    print(classification_report(y_test, predictions))\n",
    "\n",
    "# Exemple d'utilisation\n",
    "# Remplacez embedding_batch, labels par vos données réelles + labels over ou under\n",
    "X_train, X_test, y_train, y_test = train_test_split(embedding_batch, labels_undersampled, test_size=0.2, random_state=42, stratify=labels_undersampled)\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 32  # Définissez la taille du lot en fonction de votre jeu de données et de la capacité de votre machine\n",
    "\n",
    "train_and_evaluate_model(X_train, y_train, X_test, y_test, epochs, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0026693-13d2-40cb-b64d-4fc858d7785a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Training based on Vggish tuto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eed7ea7f-5216-4b84-9aaf-b602ebf00714",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qy/xk34hj_j3zz13ksmyj4_hj000000gn/T/ipykernel_22032/3125268414.py:24: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  fc1 = tf.compat.v1.layers.dense(net, 4096, activation=tf.nn.relu, name='fc1')\n",
      "/var/folders/qy/xk34hj_j3zz13ksmyj4_hj000000gn/T/ipykernel_22032/3125268414.py:26: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  logits = tf.compat.v1.layers.dense(fc1, _NUM_CLASSES, activation=tf.nn.sigmoid, name='fc2')  # Utilisation directe de sigmoid dans la couche de sortie\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: Loss 0.775765597820282\n",
      "Batch 1: Loss 0.693146288394928\n",
      "Batch 2: Loss 0.693146288394928\n",
      "Batch 3: Loss 0.693146288394928\n",
      "Batch 4: Loss 0.693146288394928\n",
      "Batch 5: Loss 0.693146288394928\n",
      "Batch 6: Loss 0.693146288394928\n",
      "Batch 7: Loss 0.693146288394928\n",
      "Batch 8: Loss 0.693146288394928\n",
      "Batch 9: Loss 0.693146288394928\n",
      "Batch 10: Loss 0.693146288394928\n",
      "Batch 11: Loss 0.693146288394928\n",
      "Batch 12: Loss 0.693146288394928\n",
      "Batch 13: Loss 0.693146288394928\n",
      "Batch 14: Loss 0.693146288394928\n",
      "Batch 15: Loss 0.693146288394928\n",
      "Batch 16: Loss 0.693146288394928\n",
      "Batch 17: Loss 0.693146288394928\n",
      "Batch 18: Loss 0.693146288394928\n",
      "Batch 19: Loss 0.693146288394928\n",
      "Batch 20: Loss 0.693146288394928\n",
      "Batch 21: Loss 0.693146288394928\n",
      "Batch 22: Loss 0.693146288394928\n",
      "Batch 23: Loss 0.693146288394928\n",
      "Batch 24: Loss 0.693146288394928\n",
      "Batch 25: Loss 0.693146288394928\n",
      "Batch 26: Loss 0.693146288394928\n",
      "Batch 27: Loss 0.693146288394928\n",
      "Batch 28: Loss 0.693146288394928\n",
      "Batch 29: Loss 0.693146288394928\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67       301\n",
      "           1       0.00      0.00      0.00       301\n",
      "\n",
      "    accuracy                           0.50       602\n",
      "   macro avg       0.25      0.50      0.33       602\n",
      "weighted avg       0.25      0.50      0.33       602\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pierrekolingba-froidevaux/anaconda3/envs/new_env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/pierrekolingba-froidevaux/anaconda3/envs/new_env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/pierrekolingba-froidevaux/anaconda3/envs/new_env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "tf.disable_v2_behavior()  # Désactiver le comportement de TensorFlow 2 pour utiliser tf.compat.v1\n",
    "\n",
    "# Définir le nombre de lots num_batches\n",
    "num_batches = 30\n",
    "_NUM_CLASSES = 1  # Pour une classification binaire\n",
    "\n",
    "def train_and_evaluate_model(X_train, y_train, X_test, y_test):\n",
    "    with tf.Graph().as_default():\n",
    "        sess = tf.Session()\n",
    "        with sess.as_default():\n",
    "            # Placeholders pour les embeddings et les labels\n",
    "            embeddings_input = tf.placeholder(tf.float32, shape=[None, X_train.shape[1]], name='embeddings_input')\n",
    "            labels_input = tf.placeholder(tf.float32, shape=[None, _NUM_CLASSES], name='labels_input')\n",
    "\n",
    "            # Definition du modèle avec une seule couche cachée\n",
    "            with tf.variable_scope('custom_layer'):\n",
    "                net = tf.identity(embeddings_input)  # Utiliser les embeddings comme entrée\n",
    "                # Couche cachée fc1\n",
    "                fc1 = tf.compat.v1.layers.dense(net, 4096, activation=tf.nn.relu, name='fc1')\n",
    "                # Couche de sortie fc2 avec activation sigmoïde\n",
    "                logits = tf.compat.v1.layers.dense(fc1, _NUM_CLASSES, activation=tf.nn.sigmoid, name='fc2')  # Utilisation directe de sigmoid dans la couche de sortie\n",
    "\n",
    "            # Loss et optimizer\n",
    "            loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels_input))       \n",
    "            train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n",
    "\n",
    "            # Initialisation des variables\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            # Assurez-vous que y_train et y_test ont la bonne forme\n",
    "            y_train = y_train.reshape(-1, 1)\n",
    "            y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "            # Entraînement\n",
    "            for i in range(num_batches):\n",
    "                _, loss_value = sess.run([train_op, loss], feed_dict={embeddings_input: X_train, labels_input: y_train})\n",
    "                print(f'Batch {i}: Loss {loss_value}')\n",
    "\n",
    "            # Calcul des logits sur l'ensemble de test\n",
    "            logits_test = sess.run(logits, feed_dict={embeddings_input: X_test})\n",
    "\n",
    "            # Convertir les logits en prédictions binaires, la conversion est inutile ici car 'logits' utilise déjà 'sigmoid'\n",
    "            predictions = (logits_test > 0.5).astype(int)\n",
    "\n",
    "            # Évaluation\n",
    "            print(classification_report(y_test, predictions))\n",
    "\n",
    "# Assurez-vous d'avoir X_train, X_test, y_train, y_test définis correctement\n",
    "train_and_evaluate_model(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29dd985c-c70b-48b4-873a-5ba5fe2ee266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quelques labels de test: [[1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Quelques labels de test:\", y_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db383944-7986-4c74-a7db-832cdcef3394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sarcasm true: 345, sarcasm not true: 345\n"
     ]
    }
   ],
   "source": [
    "# Let's first load the JSON file and then enumerate the amount of data with sarcasm = true and sarcasm = false.\n",
    "\n",
    "import json\n",
    "\n",
    "# Load the JSON data\n",
    "with open('../data/sarcasm_data.json', 'r') as file:\n",
    "    sarcasm_data = json.load(file)\n",
    "\n",
    "# Counters for sarcasm = true and sarcasm = false\n",
    "sarcasm_true_count = 0\n",
    "sarcasm_false_count = 0\n",
    "\n",
    "# Enumerate the amount of data\n",
    "for key, value in sarcasm_data.items():\n",
    "    if value[\"sarcasm\"]:\n",
    "        sarcasm_true_count += 1\n",
    "    else:\n",
    "        sarcasm_false_count += 1\n",
    "\n",
    "print(f'Sarcasm true: {sarcasm_true_count}, sarcasm not true: {sarcasm_false_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2bfc45-2a07-42fb-8e16-f265046f45f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
